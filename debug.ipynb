{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import griddata as gd\n",
    "\n",
    "net, const_load_household, const_load_heatpump, time_steps, df_household_prognosis, df_season_heatpump_prognosis, heatpump_scaling_factors_df, T_amb = gd.setup_grid_powertech25(season='winter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Household ConstControl indices:\", const_load_household.element_index)\n",
    "print(\"Heat Pump ConstControl indices:\", const_load_heatpump.element_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            158, 159, 160, 161, 162, 163, 164, 165, 166, 167],\n",
      "           dtype='int64', length=168)\n"
     ]
    }
   ],
   "source": [
    "print(time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections with edge index as an attribute\n",
    "line_edges = {}\n",
    "for idx, line in enumerate(net.line.itertuples()):\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    edge = (from_bus, to_bus)\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "    line_edges[edge] = f\"Line {idx+1}\"\n",
    "\n",
    "# Add edges for transformer connections with edge index as an attribute\n",
    "trafo_edges = {}\n",
    "for idx, trafo in enumerate(net.trafo.itertuples()):\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    edge = (hv_bus, lv_bus)\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "    trafo_edges[edge] = f\"Trafo {idx}\"\n",
    "\n",
    "# Combine all edge indices into one dictionary\n",
    "edge_labels = {**line_edges, **trafo_edges}\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels with increased vertical spacing\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "vertical_spacing = 15  # Increase vertical spacing to 15 units\n",
    "\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level * vertical_spacing  # Adjusted vertical spacing\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "\n",
    "# Add edge labels\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.5)\n",
    "\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections and Edge Indices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (replace 'your_file.csv' with the actual file path)\n",
    "file_path = 'householdPrognosis.csv'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of the data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display the header (column names) of the dataframe\n",
    "print(\"\\nColumn names (Header):\")\n",
    "print(df.columns)\n",
    "\n",
    "# Print the length of the 'dayOfWeek' column\n",
    "print(\"\\nLength of the 'dayOfWeek' column:\")\n",
    "print(df['meanP'].count())  # .count() counts non-null entries in the column\n",
    "\n",
    "# Print the last two values in the 'dayOfWeek' column\n",
    "print(\"\\nLast two values in the 'dayOfWeek' column:\")\n",
    "print(df['meanP'].tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Filter the data for rows where season is 'winter'\n",
    "winter_data = df[df['season'] == 'winter']\n",
    "\n",
    "# Convert the 'meanP' column to numeric, replacing commas with dots\n",
    "winter_data['meanP'] = winter_data['meanP'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Plot the 'meanP' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(winter_data['meanP'], marker='o', linestyle='-', color='b')\n",
    "plt.title('MeanP during Winter Season')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('MeanP')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections with edge index as an attribute\n",
    "line_edges = {}\n",
    "for idx, line in enumerate(net.line.itertuples()):\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    edge = (from_bus, to_bus)\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "    line_edges[edge] = f\"Line {idx}\"\n",
    "\n",
    "# Add edges for transformer connections with edge index as an attribute\n",
    "trafo_edges = {}\n",
    "for idx, trafo in enumerate(net.trafo.itertuples()):\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    edge = (hv_bus, lv_bus)\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "    trafo_edges[edge] = f\"Trafo {idx}\"\n",
    "\n",
    "# Combine all edge indices into one dictionary\n",
    "edge_labels = {**line_edges, **trafo_edges}\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level  # Downward hierarchy\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "\n",
    "# Add edge labels\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.5)\n",
    "\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections and Edge Indices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "# Buses and lines to remove\n",
    "buses_to_drop = [14, 34, 9, 19]\n",
    "\n",
    "# Remove lines associated with the buses to drop\n",
    "lines_to_drop = net.line[\n",
    "    (net.line.from_bus.isin(buses_to_drop)) | (net.line.to_bus.isin(buses_to_drop))\n",
    "].index\n",
    "\n",
    "# Drop the identified lines\n",
    "pp.drop_lines(net, lines_to_drop)\n",
    "\n",
    "# Drop the identified buses\n",
    "pp.drop_buses(net, buses_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections\n",
    "for line in net.line.itertuples():\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "\n",
    "# Add edges for transformer connections\n",
    "for trafo in net.trafo.itertuples():\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level  # Downward hierarchy\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant loads to 3 kW for all loads except R11 to R18\n",
    "constant_loads = net.load.index.difference([1, 2, 3, 4, 5])  # Indices of loads R1, I2, C1, C12, C13, C14, C17, C18, C19, C20\n",
    "net.load.loc[constant_loads, 'p_mw'] = 15/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConstControl controller to update values of the loads R11 to R18\n",
    "profile_loads = net.load.index.intersection([0, 1, 2, 3, 4, 5])  # Indices of loads R11 to R18\n",
    "const_load = control.ConstControl(net, element='load', element_index=profile_loads,\n",
    "                                  variable='p_mw', data_source=ds, profile_name=[\"mult\"]*len(profile_loads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a power flow for the initial condition to check if the network is properly configured\n",
    "try:\n",
    "    pp.runpp(net)\n",
    "except pp.optimal_powerflow.OPFNotConverged:\n",
    "    print(\"Initial power flow did not converge. Please check the network configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the outputwriter to save data to excel files in the current folder. You can change this to .json, .csv, or .pickle as well\n",
    "ow = timeseries.OutputWriter(net, output_path=\"./\", output_file_type=\".xlsx\")\n",
    "# adding vm_pu of all buses and line_loading in percent of all lines as outputs to be stored\n",
    "ow.log_variable('res_bus', 'vm_pu')\n",
    "ow.log_variable('res_line', 'loading_percent')\n",
    "ow.log_variable('res_load', 'p_mw')\n",
    "ow.log_variable('res_load', 'q_mvar')\n",
    "\n",
    "\n",
    "# starting the timeseries simulation for one day -> 96 15 min values.\n",
    "timeseries.run_timeseries(net)\n",
    "# now checkout the folders res_bus and res_line in your current working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "vm_pu = pd.read_excel('res_bus/vm_pu.xlsx', index_col=0)\n",
    "loading_percent = pd.read_excel('res_line/loading_percent.xlsx', index_col=0)\n",
    "load_p_mw = pd.read_excel('res_load/p_mw.xlsx', index_col=0)\n",
    "load_q_mvar = pd.read_excel('res_load/q_mvar.xlsx', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower.plotting as plot\n",
    "# Plot the grid\n",
    "plot.simple_plot(net, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate the plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(vm_pu, label=vm_pu.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Voltage Magnitude (p.u.)')\n",
    "plt.title('Voltage Magnitude over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loading_percent, label=loading_percent.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Loading Percent')\n",
    "plt.title('Line Loading over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(load_p_mw, label=load_p_mw.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Active Power Load (MW)')\n",
    "plt.title('Active Power Load over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(load_q_mvar, label=load_q_mvar.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Reactive Power Load (MVAr)')\n",
    "plt.title('Reactive Power Load over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Function to recursively explore groups and datasets\n",
    "def explore_group(group, level=0):\n",
    "    indent = \"  \" * level  # Indentation for better readability\n",
    "    for key in group.keys():\n",
    "        item = group[key]\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"{indent}Group: {key}\")\n",
    "            # Recursively explore the subgroup\n",
    "            explore_group(item, level + 1)\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"{indent}Dataset: {key}\")\n",
    "            print(f\"{indent}  Shape: {item.shape}\")\n",
    "            print(f\"{indent}  Data type: {item.dtype}\")\n",
    "            print(f\"{indent}  First 5 lines of data:\")\n",
    "            print(item[:5])  # Adjust the slicing as needed\n",
    "        else:\n",
    "            print(f\"{indent}Unknown item: {key}\")\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    print(\"Keys in the file:\", list(hdf.keys()))\n",
    "    \n",
    "    # Access the 'WEATHER_SERVICE' group\n",
    "    key = 'WEATHER_SERVICE'\n",
    "    if key in hdf:\n",
    "        group = hdf[key]\n",
    "        print(f\"Exploring '{key}' group recursively:\")\n",
    "        explore_group(group)\n",
    "    else:\n",
    "        print(f\"Key '{key}' not found in the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter5m.csv\"\n",
    "    filtered_data.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered data for the first week of January 2020 has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Set the 'index' column as the DataFrame index\n",
    "    filtered_data.set_index('index', inplace=True)\n",
    "    \n",
    "    # Resample to 15-minute intervals and calculate the mean\n",
    "    resampled_data = filtered_data.resample('15T').mean()\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter15m.csv\"\n",
    "    resampled_data.to_csv(output_file)\n",
    "    print(f\"15-minute resolution data for the first week of January 2020 has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Set the 'index' column as the DataFrame index\n",
    "    filtered_data.set_index('index', inplace=True)\n",
    "    \n",
    "    # Resample to 1-hour intervals and calculate the mean\n",
    "    resampled_data = filtered_data.resample('1H').mean()\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter1h.csv\"\n",
    "    resampled_data.to_csv(output_file)\n",
    "    print(f\"1-hour resolution data for the first week of January 2020 has been saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"heatpumpPrognosis.csv\"  # Replace with the correct path to your CSV file\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few lines\n",
    "print(\"First few lines of the 1-hour resolution data:\")\n",
    "print(df.head())\n",
    "# Print the length of the DataFrame\n",
    "print(f\"The number of rows in the DataFrame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = \"heatpumpPrognosis.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path, sep=\";\")  # Adjust the separator if needed\n",
    "\n",
    "# Clean and convert numeric columns safely\n",
    "for col in ['meanP', 'stdP', 'meanQ', 'stdQ']:\n",
    "    # Replace commas with dots and remove whitespace\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)  # Ensure all values are strings\n",
    "        .str.replace(r'\\s+', '', regex=True)  # Remove spaces\n",
    "        .str.replace(',', '.')  # Replace commas with dots\n",
    "    )\n",
    "    # Convert to float\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Check for NaN values after conversion\n",
    "if df[['meanP', 'stdP', 'meanQ', 'stdQ']].isnull().any().any():\n",
    "    print(\"Warning: Some values could not be converted to numeric and are NaN.\")\n",
    "\n",
    "# Preserve the custom order of seasons\n",
    "season_order = ['winter', 'summer', 'interim']\n",
    "df['season'] = pd.Categorical(df['season'], categories=season_order, ordered=True)\n",
    "\n",
    "# Group by 'season', 'dayOfWeek', and 'hour' (discard 'minute' for 1-hour resolution)\n",
    "resampled_df = df.groupby(['season', 'dayOfWeek', 'hour']).mean().reset_index()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "resampled_df = resampled_df.drop(columns=['minute', 'Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Sort the DataFrame to ensure the order of seasons is maintained\n",
    "resampled_df = resampled_df.sort_values(by=['season', 'dayOfWeek', 'hour'])\n",
    "\n",
    "# Export the resampled data to a new CSV file\n",
    "output_file = \"heatpumpPrognosis1h.csv\"\n",
    "resampled_df.to_csv(output_file, sep=\";\", index=False)\n",
    "print(f\"Resampled data has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"temperatureWinter1h.csv\"  # Replace with the correct path to your CSV file\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few lines\n",
    "print(\"First few lines of the 1-hour resolution data:\")\n",
    "print(df.head())\n",
    "# Print the length of the DataFrame\n",
    "print(f\"The number of rows in the DataFrame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_amb = df['APPARENT_TEMPERATURE:TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T_amb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pandapipes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
