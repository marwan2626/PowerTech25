{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import griddata as gd\n",
    "\n",
    "net, const_load_household, const_load_heatpump, time_steps, df_household_prognosis, df_season_heatpump_prognosis, heatpump_scaling_factors_df, T_amb = gd.setup_grid_powertech25(season='winter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Household ConstControl indices:\", const_load_household.element_index)\n",
    "print(\"Heat Pump ConstControl indices:\", const_load_heatpump.element_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>std_type</th>\n",
       "      <th>hv_bus</th>\n",
       "      <th>lv_bus</th>\n",
       "      <th>sn_mva</th>\n",
       "      <th>vn_hv_kv</th>\n",
       "      <th>vn_lv_kv</th>\n",
       "      <th>vk_percent</th>\n",
       "      <th>vkr_percent</th>\n",
       "      <th>pfe_kw</th>\n",
       "      <th>i0_percent</th>\n",
       "      <th>shift_degree</th>\n",
       "      <th>tap_side</th>\n",
       "      <th>tap_neutral</th>\n",
       "      <th>tap_min</th>\n",
       "      <th>tap_max</th>\n",
       "      <th>tap_step_percent</th>\n",
       "      <th>tap_step_degree</th>\n",
       "      <th>tap_pos</th>\n",
       "      <th>tap_phase_shifter</th>\n",
       "      <th>parallel</th>\n",
       "      <th>df</th>\n",
       "      <th>in_service</th>\n",
       "      <th>substation</th>\n",
       "      <th>max_loading_percent</th>\n",
       "      <th>autoTapSetp</th>\n",
       "      <th>voltLvl</th>\n",
       "      <th>autoTapSide</th>\n",
       "      <th>subnet</th>\n",
       "      <th>autoTap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MV1.101-LV4.101-Trafo 1</td>\n",
       "      <td>0.4 MVA 20/0.4 kV Dyn5 ASEA</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.30001</td>\n",
       "      <td>150.0</td>\n",
       "      <td>hv</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LV4.101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                     std_type  hv_bus  lv_bus  \\\n",
       "0  MV1.101-LV4.101-Trafo 1  0.4 MVA 20/0.4 kV Dyn5 ASEA       0      15   \n",
       "\n",
       "   sn_mva  vn_hv_kv  vn_lv_kv  vk_percent  vkr_percent  pfe_kw  i0_percent  \\\n",
       "0     0.4      20.0       0.4         6.0          1.2     1.2     0.30001   \n",
       "\n",
       "   shift_degree tap_side  tap_neutral  tap_min  tap_max  tap_step_percent  \\\n",
       "0         150.0       hv          0.0     -2.0      2.0               2.5   \n",
       "\n",
       "   tap_step_degree  tap_pos  tap_phase_shifter  parallel   df  in_service  \\\n",
       "0              0.0        0              False         1  1.0        True   \n",
       "\n",
       "  substation  max_loading_percent autoTapSetp voltLvl autoTapSide   subnet  \\\n",
       "0        NaN                100.0         NaN       6         NaN  LV4.101   \n",
       "\n",
       "  autoTap  \n",
       "0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.trafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections with edge index as an attribute\n",
    "line_edges = {}\n",
    "for idx, line in enumerate(net.line.itertuples()):\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    edge = (from_bus, to_bus)\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "    line_edges[edge] = f\"Line {idx+1}\"\n",
    "\n",
    "# Add edges for transformer connections with edge index as an attribute\n",
    "trafo_edges = {}\n",
    "for idx, trafo in enumerate(net.trafo.itertuples()):\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    edge = (hv_bus, lv_bus)\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "    trafo_edges[edge] = f\"Trafo {idx}\"\n",
    "\n",
    "# Combine all edge indices into one dictionary\n",
    "edge_labels = {**line_edges, **trafo_edges}\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels with increased vertical spacing\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "vertical_spacing = 15  # Increase vertical spacing to 15 units\n",
    "\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level * vertical_spacing  # Adjusted vertical spacing\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "\n",
    "# Add edge labels\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.5)\n",
    "\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections and Edge Indices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (replace 'your_file.csv' with the actual file path)\n",
    "file_path = 'householdPrognosis.csv'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of the data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display the header (column names) of the dataframe\n",
    "print(\"\\nColumn names (Header):\")\n",
    "print(df.columns)\n",
    "\n",
    "# Print the length of the 'dayOfWeek' column\n",
    "print(\"\\nLength of the 'dayOfWeek' column:\")\n",
    "print(df['meanP'].count())  # .count() counts non-null entries in the column\n",
    "\n",
    "# Print the last two values in the 'dayOfWeek' column\n",
    "print(\"\\nLast two values in the 'dayOfWeek' column:\")\n",
    "print(df['meanP'].tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Filter the data for rows where season is 'winter'\n",
    "winter_data = df[df['season'] == 'winter']\n",
    "\n",
    "# Convert the 'meanP' column to numeric, replacing commas with dots\n",
    "winter_data['meanP'] = winter_data['meanP'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Plot the 'meanP' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(winter_data['meanP'], marker='o', linestyle='-', color='b')\n",
    "plt.title('MeanP during Winter Season')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('MeanP')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections with edge index as an attribute\n",
    "line_edges = {}\n",
    "for idx, line in enumerate(net.line.itertuples()):\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    edge = (from_bus, to_bus)\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "    line_edges[edge] = f\"Line {idx}\"\n",
    "\n",
    "# Add edges for transformer connections with edge index as an attribute\n",
    "trafo_edges = {}\n",
    "for idx, trafo in enumerate(net.trafo.itertuples()):\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    edge = (hv_bus, lv_bus)\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "    trafo_edges[edge] = f\"Trafo {idx}\"\n",
    "\n",
    "# Combine all edge indices into one dictionary\n",
    "edge_labels = {**line_edges, **trafo_edges}\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level  # Downward hierarchy\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "\n",
    "# Add edge labels\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.5)\n",
    "\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections and Edge Indices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "# Buses and lines to remove\n",
    "buses_to_drop = [14, 34, 9, 19]\n",
    "\n",
    "# Remove lines associated with the buses to drop\n",
    "lines_to_drop = net.line[\n",
    "    (net.line.from_bus.isin(buses_to_drop)) | (net.line.to_bus.isin(buses_to_drop))\n",
    "].index\n",
    "\n",
    "# Drop the identified lines\n",
    "pp.drop_lines(net, lines_to_drop)\n",
    "\n",
    "# Drop the identified buses\n",
    "pp.drop_buses(net, buses_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all buses as nodes\n",
    "for bus in net.bus.index:\n",
    "    G.add_node(bus)\n",
    "\n",
    "# Add edges based on line connections\n",
    "for line in net.line.itertuples():\n",
    "    from_bus = line.from_bus\n",
    "    to_bus = line.to_bus\n",
    "    G.add_edge(from_bus, to_bus)\n",
    "\n",
    "# Add edges for transformer connections\n",
    "for trafo in net.trafo.itertuples():\n",
    "    hv_bus = trafo.hv_bus\n",
    "    lv_bus = trafo.lv_bus\n",
    "    G.add_edge(hv_bus, lv_bus)\n",
    "\n",
    "# Find the root bus (e.g., the slack bus or transformer bus)\n",
    "root_bus = net.ext_grid.bus.iloc[0]\n",
    "\n",
    "# Debugging: Ensure the root bus exists in the graph\n",
    "if root_bus not in G.nodes:\n",
    "    print(f\"Warning: Root bus {root_bus} is not in the graph.\")\n",
    "    print(f\"Available nodes: {list(G.nodes)}\")\n",
    "else:\n",
    "    print(f\"Root bus {root_bus} found.\")\n",
    "\n",
    "# Generate a topological sort to ensure hierarchy\n",
    "levels = {node: 0 for node in G.nodes}\n",
    "visited = set()\n",
    "\n",
    "def assign_levels(node, level):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    levels[node] = level\n",
    "    for neighbor in G.neighbors(node):\n",
    "        assign_levels(neighbor, level + 1)\n",
    "\n",
    "# Assign levels starting from the root bus\n",
    "assign_levels(root_bus, 0)\n",
    "\n",
    "# Define a position based on levels\n",
    "pos = {}\n",
    "nodes_by_level = {}\n",
    "for node, level in levels.items():\n",
    "    if level not in nodes_by_level:\n",
    "        nodes_by_level[level] = []\n",
    "    nodes_by_level[level].append(node)\n",
    "\n",
    "for level, nodes in nodes_by_level.items():\n",
    "    y = -level  # Downward hierarchy\n",
    "    x_positions = range(len(nodes))\n",
    "    for x, node in zip(x_positions, nodes):\n",
    "        pos[node] = (x, y)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=500,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    arrowsize=10,\n",
    ")\n",
    "\n",
    "# Highlight the root bus\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[root_bus], node_color=\"red\", node_size=700)\n",
    "plt.title(\"Hierarchical Representation of the Grid with Transformer Connections\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant loads to 3 kW for all loads except R11 to R18\n",
    "constant_loads = net.load.index.difference([1, 2, 3, 4, 5])  # Indices of loads R1, I2, C1, C12, C13, C14, C17, C18, C19, C20\n",
    "net.load.loc[constant_loads, 'p_mw'] = 15/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConstControl controller to update values of the loads R11 to R18\n",
    "profile_loads = net.load.index.intersection([0, 1, 2, 3, 4, 5])  # Indices of loads R11 to R18\n",
    "const_load = control.ConstControl(net, element='load', element_index=profile_loads,\n",
    "                                  variable='p_mw', data_source=ds, profile_name=[\"mult\"]*len(profile_loads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a power flow for the initial condition to check if the network is properly configured\n",
    "try:\n",
    "    pp.runpp(net)\n",
    "except pp.optimal_powerflow.OPFNotConverged:\n",
    "    print(\"Initial power flow did not converge. Please check the network configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the outputwriter to save data to excel files in the current folder. You can change this to .json, .csv, or .pickle as well\n",
    "ow = timeseries.OutputWriter(net, output_path=\"./\", output_file_type=\".xlsx\")\n",
    "# adding vm_pu of all buses and line_loading in percent of all lines as outputs to be stored\n",
    "ow.log_variable('res_bus', 'vm_pu')\n",
    "ow.log_variable('res_line', 'loading_percent')\n",
    "ow.log_variable('res_load', 'p_mw')\n",
    "ow.log_variable('res_load', 'q_mvar')\n",
    "\n",
    "\n",
    "# starting the timeseries simulation for one day -> 96 15 min values.\n",
    "timeseries.run_timeseries(net)\n",
    "# now checkout the folders res_bus and res_line in your current working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "vm_pu = pd.read_excel('res_bus/vm_pu.xlsx', index_col=0)\n",
    "loading_percent = pd.read_excel('res_line/loading_percent.xlsx', index_col=0)\n",
    "load_p_mw = pd.read_excel('res_load/p_mw.xlsx', index_col=0)\n",
    "load_q_mvar = pd.read_excel('res_load/q_mvar.xlsx', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower.plotting as plot\n",
    "# Plot the grid\n",
    "plot.simple_plot(net, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate the plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(vm_pu, label=vm_pu.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Voltage Magnitude (p.u.)')\n",
    "plt.title('Voltage Magnitude over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loading_percent, label=loading_percent.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Loading Percent')\n",
    "plt.title('Line Loading over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(load_p_mw, label=load_p_mw.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Active Power Load (MW)')\n",
    "plt.title('Active Power Load over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(load_q_mvar, label=load_q_mvar.columns)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Reactive Power Load (MVAr)')\n",
    "plt.title('Reactive Power Load over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['WEATHER_SERVICE']\n",
      "Exploring 'WEATHER_SERVICE' group recursively:\n",
      "Group: IN\n",
      "  Group: WEATHER_APPARENT_TEMPERATURE_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('APPARENT_TEMPERATURE:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, -0.5) (1577833500000000000, -0.5)\n",
      " (1577833800000000000, -1. ) (1577834100000000000, -1. )\n",
      " (1577834400000000000, -1. )]\n",
      "  Group: WEATHER_ATMOSPHERIC_PRESSURE_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('ATMOSPHERIC_PRESSURE:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 1035.5) (1577833500000000000, 1035.5)\n",
      " (1577833800000000000, 1035.5) (1577834100000000000, 1035.5)\n",
      " (1577834400000000000, 1035.5)]\n",
      "  Group: WEATHER_PRECIPITATION_RATE_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('PRECIPITATION_RATE:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 0.) (1577833500000000000, 0.)\n",
      " (1577833800000000000, 0.) (1577834100000000000, 0.)\n",
      " (1577834400000000000, 0.)]\n",
      "  Group: WEATHER_PROBABILITY_OF_PRECIPITATION_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('PROBABILITY_OF_PRECIPITATION:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 10.) (1577833500000000000, 10.)\n",
      " (1577833800000000000, 10.) (1577834100000000000, 10.)\n",
      " (1577834400000000000, 10.)]\n",
      "  Group: WEATHER_RELATIVE_HUMIDITY_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('RELATIVE_HUMIDITY:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 90.) (1577833500000000000, 90.)\n",
      " (1577833800000000000, 90.) (1577834100000000000, 90.)\n",
      " (1577834400000000000, 90.)]\n",
      "  Group: WEATHER_SOLAR_IRRADIANCE_GLOBAL\n",
      "    Dataset: table\n",
      "      Shape: (102274,)\n",
      "      Data type: [('index', '<i8'), ('SOLAR_IRRADIANCE:GLOBAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 0.) (1577833500000000000, 0.)\n",
      " (1577833800000000000, 0.) (1577834100000000000, 0.)\n",
      " (1577834400000000000, 0.)]\n",
      "  Group: WEATHER_TEMPERATURE_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('TEMPERATURE:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 1.10000002) (1577833500000000000, 1.10000002)\n",
      " (1577833800000000000, 0.69999999) (1577834100000000000, 0.69999999)\n",
      " (1577834400000000000, 0.69999999)]\n",
      "  Group: WEATHER_WIND_DIRECTION_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('WIND_DIRECTION:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 225.) (1577833500000000000, 225.)\n",
      " (1577833800000000000, 230.) (1577834100000000000, 230.)\n",
      " (1577834400000000000, 225.)]\n",
      "  Group: WEATHER_WIND_GUST_SPEED_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102470,)\n",
      "      Data type: [('index', '<i8'), ('WIND_GUST_SPEED:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 3.05555558) (1577833500000000000, 3.05555558)\n",
      " (1577833800000000000, 3.05555558) (1577834100000000000, 3.05555558)\n",
      " (1577834400000000000, 3.05555558)]\n",
      "  Group: WEATHER_WIND_SPEED_TOTAL\n",
      "    Dataset: table\n",
      "      Shape: (102274,)\n",
      "      Data type: [('index', '<i8'), ('WIND_SPEED:TOTAL', '<f8')]\n",
      "      First 5 lines of data:\n",
      "[(1577833200000000000, 1.5) (1577833500000000000, 1.5)\n",
      " (1577833800000000000, 1.5) (1577834100000000000, 1.5)\n",
      " (1577834400000000000, 1.5)]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Function to recursively explore groups and datasets\n",
    "def explore_group(group, level=0):\n",
    "    indent = \"  \" * level  # Indentation for better readability\n",
    "    for key in group.keys():\n",
    "        item = group[key]\n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(f\"{indent}Group: {key}\")\n",
    "            # Recursively explore the subgroup\n",
    "            explore_group(item, level + 1)\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(f\"{indent}Dataset: {key}\")\n",
    "            print(f\"{indent}  Shape: {item.shape}\")\n",
    "            print(f\"{indent}  Data type: {item.dtype}\")\n",
    "            print(f\"{indent}  First 5 lines of data:\")\n",
    "            print(item[:5])  # Adjust the slicing as needed\n",
    "        else:\n",
    "            print(f\"{indent}Unknown item: {key}\")\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    print(\"Keys in the file:\", list(hdf.keys()))\n",
    "    \n",
    "    # Access the 'WEATHER_SERVICE' group\n",
    "    key = 'WEATHER_SERVICE'\n",
    "    if key in hdf:\n",
    "        group = hdf[key]\n",
    "        print(f\"Exploring '{key}' group recursively:\")\n",
    "        explore_group(group)\n",
    "    else:\n",
    "        print(f\"Key '{key}' not found in the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for the first week of January 2020 has been saved to 'temperatureWinter5m.csv'.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter5m.csv\"\n",
    "    filtered_data.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered data for the first week of January 2020 has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-minute resolution data for the first week of January 2020 has been saved to 'temperatureWinter15m.csv'.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Set the 'index' column as the DataFrame index\n",
    "    filtered_data.set_index('index', inplace=True)\n",
    "    \n",
    "    # Resample to 15-minute intervals and calculate the mean\n",
    "    resampled_data = filtered_data.resample('15T').mean()\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter15m.csv\"\n",
    "    resampled_data.to_csv(output_file)\n",
    "    print(f\"15-minute resolution data for the first week of January 2020 has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hour resolution data for the first week of January 2020 has been saved to 'temperatureWinter1h.csv'.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = \"2020_weather.hdf5\"  # Replace with the path to your HDF5 file\n",
    "with h5py.File(file_path, \"r\") as hdf:\n",
    "    # Access the dataset for apparent temperature\n",
    "    dataset_path = \"WEATHER_SERVICE/IN/WEATHER_APPARENT_TEMPERATURE_TOTAL/table\"\n",
    "    dataset = hdf[dataset_path]\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset[:])  # Convert structured array to DataFrame\n",
    "    # Convert 'index' field (timestamp) to datetime\n",
    "    data['index'] = pd.to_datetime(data['index'], unit='ns')\n",
    "    \n",
    "    # Filter for the first week of January 2020\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2020-01-08\"\n",
    "    filtered_data = data[(data['index'] >= start_date) & (data['index'] < end_date)]\n",
    "    \n",
    "    # Set the 'index' column as the DataFrame index\n",
    "    filtered_data.set_index('index', inplace=True)\n",
    "    \n",
    "    # Resample to 1-hour intervals and calculate the mean\n",
    "    resampled_data = filtered_data.resample('1H').mean()\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_file = \"temperatureWinter1h.csv\"\n",
    "    resampled_data.to_csv(output_file)\n",
    "    print(f\"1-hour resolution data for the first week of January 2020 has been saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of the 1-hour resolution data:\n",
      "                                                                            ;season;dayOfWeek;hour;minute;meanP;stdP;meanQ;stdQ\n",
      "0;winter;0;0;0;12759  525390625;1503  987548828125;-126 24398040771484;249                                      8841094970703  \n",
      "1;winter;0;0;15;12574 7275390625;1483 3095703125;-143   9138641357422;245                                      64804077148438  \n",
      "2;winter;0;0;30;12450 158203125;1503  919677734375;-133 84373474121094;252                                     56521606445312  \n",
      "3;winter;0;0;45;12458 5205078125;1487 131591796875;-141 7138214111328;247                                      40904235839844  \n",
      "4;winter;0;1;0;12496  75390625;1485   060546875;-133    11349487304688;242                                     23989868164062  \n",
      "The number of rows in the DataFrame: 2016\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"heatpumpPrognosis.csv\"  # Replace with the correct path to your CSV file\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few lines\n",
    "print(\"First few lines of the 1-hour resolution data:\")\n",
    "print(df.head())\n",
    "# Print the length of the DataFrame\n",
    "print(f\"The number of rows in the DataFrame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled data has been saved to 'heatpumpPrognosis1h.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = \"heatpumpPrognosis.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path, sep=\";\")  # Adjust the separator if needed\n",
    "\n",
    "# Clean and convert numeric columns safely\n",
    "for col in ['meanP', 'stdP', 'meanQ', 'stdQ']:\n",
    "    # Replace commas with dots and remove whitespace\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)  # Ensure all values are strings\n",
    "        .str.replace(r'\\s+', '', regex=True)  # Remove spaces\n",
    "        .str.replace(',', '.')  # Replace commas with dots\n",
    "    )\n",
    "    # Convert to float\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Check for NaN values after conversion\n",
    "if df[['meanP', 'stdP', 'meanQ', 'stdQ']].isnull().any().any():\n",
    "    print(\"Warning: Some values could not be converted to numeric and are NaN.\")\n",
    "\n",
    "# Preserve the custom order of seasons\n",
    "season_order = ['winter', 'summer', 'interim']\n",
    "df['season'] = pd.Categorical(df['season'], categories=season_order, ordered=True)\n",
    "\n",
    "# Group by 'season', 'dayOfWeek', and 'hour' (discard 'minute' for 1-hour resolution)\n",
    "resampled_df = df.groupby(['season', 'dayOfWeek', 'hour']).mean().reset_index()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "resampled_df = resampled_df.drop(columns=['minute', 'Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Sort the DataFrame to ensure the order of seasons is maintained\n",
    "resampled_df = resampled_df.sort_values(by=['season', 'dayOfWeek', 'hour'])\n",
    "\n",
    "# Export the resampled data to a new CSV file\n",
    "output_file = \"heatpumpPrognosis1h.csv\"\n",
    "resampled_df.to_csv(output_file, sep=\";\", index=False)\n",
    "print(f\"Resampled data has been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of the 1-hour resolution data:\n",
      "                 index  APPARENT_TEMPERATURE:TOTAL\n",
      "0  2020-01-01 00:00:00                   -0.683333\n",
      "1  2020-01-01 01:00:00                   -1.100000\n",
      "2  2020-01-01 02:00:00                   -1.391667\n",
      "3  2020-01-01 03:00:00                   -1.558333\n",
      "4  2020-01-01 04:00:00                   -1.833333\n",
      "The number of rows in the DataFrame: 168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"temperatureWinter1h.csv\"  # Replace with the correct path to your CSV file\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few lines\n",
    "print(\"First few lines of the 1-hour resolution data:\")\n",
    "print(df.head())\n",
    "# Print the length of the DataFrame\n",
    "print(f\"The number of rows in the DataFrame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_amb = df['APPARENT_TEMPERATURE:TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     -0.683333\n",
      "1     -1.100000\n",
      "2     -1.391667\n",
      "3     -1.558333\n",
      "4     -1.833333\n",
      "         ...   \n",
      "163    1.550000\n",
      "164    1.058333\n",
      "165    1.700000\n",
      "166    2.541667\n",
      "167    2.900000\n",
      "Name: APPARENT_TEMPERATURE:TOTAL, Length: 168, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(T_amb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pandapipes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
